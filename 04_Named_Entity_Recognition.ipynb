{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to NER](#introduction)\n",
    "2. [Understanding Named Entities](#understanding)\n",
    "3. [NER with NLTK](#nltk-ner)\n",
    "4. [NER with spaCy](#spacy-ner)\n",
    "5. [Entity Types](#entity-types)\n",
    "6. [Applications of NER](#applications)\n",
    "7. [Advanced Topics](#advanced)\n",
    "8. [Real-World Examples](#real-world)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to NER <a id='introduction'></a>\n",
    "\n",
    "**Named Entity Recognition (NER)** is the task of identifying and classifying named entities in text into predefined categories.\n",
    "\n",
    "### What is a Named Entity?\n",
    "\n",
    "A **named entity** is a real-world object with a proper name, such as:\n",
    "- **Person**: Barack Obama, Marie Curie\n",
    "- **Organization**: Google, United Nations\n",
    "- **Location**: Paris, Mount Everest\n",
    "- **Date**: January 1, 2024\n",
    "- **Time**: 3:00 PM\n",
    "- **Money**: $100, €50\n",
    "- **Percentage**: 25%, 3.14%\n",
    "\n",
    "### Example:\n",
    "\n",
    "```\n",
    "Text: \"Apple Inc. was founded by Steve Jobs in Cupertino in 1976.\"\n",
    "\n",
    "Named Entities:\n",
    "  - \"Apple Inc.\" → ORGANIZATION\n",
    "  - \"Steve Jobs\" → PERSON\n",
    "  - \"Cupertino\" → LOCATION (GPE - Geopolitical Entity)\n",
    "  - \"1976\" → DATE\n",
    "```\n",
    "\n",
    "### Why is NER Important?\n",
    "\n",
    "1. **Information Extraction**\n",
    "   - Extract structured information from unstructured text\n",
    "   - Build knowledge graphs\n",
    "\n",
    "2. **Question Answering**\n",
    "   - \"Who founded Apple?\" → Need to identify person entities\n",
    "\n",
    "3. **Content Classification**\n",
    "   - Categorize articles by mentioned entities\n",
    "\n",
    "4. **Search Enhancement**\n",
    "   - Improve search by recognizing entity types\n",
    "\n",
    "5. **Recommendation Systems**\n",
    "   - Recommend articles mentioning similar entities\n",
    "\n",
    "### NER Pipeline:\n",
    "\n",
    "```\n",
    "Raw Text → Tokenization → POS Tagging → NER → Entity Classification\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import necessary libraries\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('maxent_ne_chunker_tab', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Named Entities <a id='understanding'></a>\n",
    "\n",
    "### Common Entity Types:\n",
    "\n",
    "| Entity Type | Description | Examples |\n",
    "|-------------|-------------|----------|\n",
    "| **PERSON** | People's names | Barack Obama, Marie Curie |\n",
    "| **ORGANIZATION (ORG)** | Companies, agencies, institutions | Google, NASA, Harvard |\n",
    "| **GPE** | Geopolitical entities (countries, cities) | USA, Paris, California |\n",
    "| **LOCATION (LOC)** | Non-GPE locations | Mount Everest, Pacific Ocean |\n",
    "| **DATE** | Absolute or relative dates | January 1, yesterday, 2024 |\n",
    "| **TIME** | Times | 3:00 PM, morning |\n",
    "| **MONEY** | Monetary values | $100, €50, £30 |\n",
    "| **PERCENT** | Percentages | 25%, 0.5% |\n",
    "| **PRODUCT** | Products | iPhone, Windows |\n",
    "| **EVENT** | Named events | Olympics, World War II |\n",
    "| **LANGUAGE** | Languages | English, Spanish |\n",
    "| **WORK_OF_ART** | Books, songs, etc. | \"Mona Lisa\", \"Hamlet\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NER with NLTK <a id='nltk-ner'></a>\n",
    "\n",
    "NLTK provides basic NER functionality using `ne_chunk()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic NER with NLTK\n",
    "\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\"\n",
    "\n",
    "print(f\"Text: '{text}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"\\n1. Tokens: {tokens}\")\n",
    "\n",
    "# Step 2: POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(f\"\\n2. POS Tags: {pos_tags}\")\n",
    "\n",
    "# Step 3: Named Entity Recognition\n",
    "# binary=False returns entity types (PERSON, ORGANIZATION, etc.)\n",
    "# binary=True just marks whether it's a named entity or not\n",
    "named_entities = ne_chunk(pos_tags, binary=False)\n",
    "\n",
    "print(f\"\\n3. Named Entities (tree structure):\")\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract entities in a more readable format\n",
    "\n",
    "def extract_entities_nltk(text):\n",
    "    \"\"\"\n",
    "    Extract named entities using NLTK.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (entity_text, entity_type) tuples\n",
    "    \"\"\"\n",
    "    # Tokenize and POS tag\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Named entity recognition\n",
    "    chunks = ne_chunk(pos_tags, binary=False)\n",
    "    \n",
    "    entities = []\n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            # This is a named entity\n",
    "            entity_text = ' '.join(c[0] for c in chunk)\n",
    "            entity_type = chunk.label()\n",
    "            entities.append((entity_text, entity_type))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Test on sample text\n",
    "text = \"\"\"Microsoft was founded by Bill Gates and Paul Allen in Seattle. \n",
    "The company is now headquartered in Redmond, Washington. \n",
    "Satya Nadella became CEO in February 2014.\"\"\"\n",
    "\n",
    "entities = extract_entities_nltk(text)\n",
    "\n",
    "print(\"Extracted Named Entities (NLTK):\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(\"-\"*80)\n",
    "print(f\"\\n{'Entity':<30} {'Type':<20}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for entity, entity_type in entities:\n",
    "    print(f\"{entity:<30} {entity_type:<20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative format using tree2conlltags\n",
    "\n",
    "def extract_entities_with_iob(text):\n",
    "    \"\"\"\n",
    "    Extract entities in IOB (Inside-Outside-Beginning) format.\n",
    "    IOB format marks the boundaries of chunks.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    chunks = ne_chunk(pos_tags)\n",
    "    \n",
    "    # Convert tree to IOB tags\n",
    "    iob_tags = tree2conlltags(chunks)\n",
    "    \n",
    "    return iob_tags\n",
    "\n",
    "# Example\n",
    "text = \"Barack Obama was born in Hawaii and became the 44th President of the United States.\"\n",
    "\n",
    "iob_tags = extract_entities_with_iob(text)\n",
    "\n",
    "print(\"IOB Format Tagging:\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(f\"{'Word':<20} {'POS':<10} {'IOB Tag':<15}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for word, pos, iob in iob_tags:\n",
    "    print(f\"{word:<20} {pos:<10} {iob:<15}\")\n",
    "\n",
    "print(\"\\nIOB Tag Explanation:\")\n",
    "print(\"  O         = Outside (not a named entity)\")\n",
    "print(\"  B-TYPE    = Beginning of entity of TYPE\")\n",
    "print(\"  I-TYPE    = Inside (continuation) of entity of TYPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NER with spaCy <a id='spacy-ner'></a>\n",
    "\n",
    "spaCy provides more accurate and comprehensive NER with more entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic NER with spaCy\n",
    "\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California in 1976.\"\n",
    "\n",
    "print(f\"Text: '{text}'\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Process with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract entities\n",
    "print(\"\\nNamed Entities (spaCy):\\n\")\n",
    "print(f\"{'Entity':<25} {'Type':<15} {'Description'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<25} {ent.label_:<15} {spacy.explain(ent.label_)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nEntity Details:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"  '{ent.text}' → {ent.label_} (characters {ent.start_char}-{ent.end_char})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More comprehensive example\n",
    "\n",
    "text = \"\"\"On January 15, 2024, Elon Musk announced that Tesla would invest \n",
    "$5 billion in a new factory in Austin, Texas. The stock price rose by 12.5% \n",
    "following the announcement. SpaceX, another company founded by Musk, also \n",
    "reported record revenues of €2.3 billion last quarter.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Comprehensive NER Example:\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Group entities by type\n",
    "entities_by_type = {}\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ not in entities_by_type:\n",
    "        entities_by_type[ent.label_] = []\n",
    "    entities_by_type[ent.label_].append(ent.text)\n",
    "\n",
    "# Display grouped entities\n",
    "for entity_type, entities in sorted(entities_by_type.items()):\n",
    "    print(f\"\\n{entity_type} ({spacy.explain(entity_type)}):\")\n",
    "    for entity in entities:\n",
    "        print(f\"  - {entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare NLTK vs spaCy\n",
    "\n",
    "test_text = \"\"\"Google CEO Sundar Pichai announced a $10 billion investment in AI \n",
    "research at the company's headquarters in Mountain View, California on March 1, 2024.\"\"\"\n",
    "\n",
    "print(\"NLTK vs spaCy Comparison:\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Text: {test_text}\\n\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# NLTK entities\n",
    "nltk_entities = extract_entities_nltk(test_text)\n",
    "print(\"\\nNLTK Entities:\")\n",
    "for entity, etype in nltk_entities:\n",
    "    print(f\"  {entity:<30} → {etype}\")\n",
    "\n",
    "# spaCy entities\n",
    "spacy_doc = nlp(test_text)\n",
    "print(\"\\nspaCy Entities:\")\n",
    "for ent in spacy_doc.ents:\n",
    "    print(f\"  {ent.text:<30} → {ent.label_}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  - spaCy generally provides more accurate and detailed entity recognition\")\n",
    "print(\"  - spaCy recognizes more entity types (MONEY, DATE, etc.)\")\n",
    "print(\"  - NLTK is simpler but less comprehensive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entity Types <a id='entity-types'></a>\n",
    "\n",
    "Let's explore different entity types in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of different entity types\n",
    "\n",
    "examples = {\n",
    "    'PERSON': \"Albert Einstein, Marie Curie, and Isaac Newton were brilliant scientists.\",\n",
    "    'ORG': \"Google, Microsoft, and Amazon are leading tech companies.\",\n",
    "    'GPE': \"Paris, London, and Tokyo are major world cities.\",\n",
    "    'MONEY': \"The house costs $500,000 or approximately €450,000.\",\n",
    "    'DATE': \"The meeting is scheduled for January 15, 2024, next Monday.\",\n",
    "    'TIME': \"We'll meet at 3:30 PM or around noon tomorrow.\",\n",
    "    'PERCENT': \"The stock rose by 15.5% and fell by 2.3% the next day.\",\n",
    "    'PRODUCT': \"I bought the new iPhone 15 Pro and a MacBook Air.\",\n",
    "}\n",
    "\n",
    "print(\"Entity Type Examples:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for target_type, text in examples.items():\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    print(f\"\\n{target_type}:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Entities found:\")\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == target_type or (target_type == 'GPE' and ent.label_ in ['GPE', 'LOC']):\n",
    "            print(f\"  ✓ '{ent.text}' → {ent.label_}\")\n",
    "    \n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all available entity types in spaCy\n",
    "\n",
    "# Process a diverse text\n",
    "diverse_text = \"\"\"On June 15, 2024, at 10:00 AM, President Joe Biden met with \n",
    "Japanese Prime Minister at the White House in Washington, D.C. They discussed \n",
    "a $500 million trade agreement. The Nikkei 225 rose by 3.2% while the S&P 500 \n",
    "gained 1.8%. Apple Inc. and Toyota Motor Corporation signed a partnership deal.\"\"\"\n",
    "\n",
    "doc = nlp(diverse_text)\n",
    "\n",
    "# Get unique entity types\n",
    "entity_types = set([ent.label_ for ent in doc.ents])\n",
    "\n",
    "print(\"All Entity Types Found:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for etype in sorted(entity_types):\n",
    "    explanation = spacy.explain(etype)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ == etype]\n",
    "    print(f\"\\n{etype}: {explanation}\")\n",
    "    print(f\"  Examples: {', '.join(entities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Applications of NER <a id='applications'></a>\n",
    "\n",
    "Real-world applications of Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 1: Information Extraction from News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key information from a news article\n",
    "\n",
    "news_article = \"\"\"In a major development, OpenAI announced on December 10, 2024, that \n",
    "its latest language model has achieved unprecedented results. CEO Sam Altman stated \n",
    "that the company invested over $100 million in research and development. The \n",
    "announcement was made at the company's headquarters in San Francisco, California. \n",
    "Shares of Microsoft, a major investor in OpenAI, rose by 5.6% following the news. \n",
    "The technology is expected to be integrated into products by March 2025.\"\"\"\n",
    "\n",
    "doc = nlp(news_article)\n",
    "\n",
    "print(\"News Article Analysis:\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Article: {news_article[:100]}...\\n\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Extract structured information\n",
    "info = {\n",
    "    'Organizations': [],\n",
    "    'People': [],\n",
    "    'Locations': [],\n",
    "    'Dates': [],\n",
    "    'Money': [],\n",
    "    'Percentages': []\n",
    "}\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'ORG':\n",
    "        info['Organizations'].append(ent.text)\n",
    "    elif ent.label_ == 'PERSON':\n",
    "        info['People'].append(ent.text)\n",
    "    elif ent.label_ in ['GPE', 'LOC']:\n",
    "        info['Locations'].append(ent.text)\n",
    "    elif ent.label_ == 'DATE':\n",
    "        info['Dates'].append(ent.text)\n",
    "    elif ent.label_ == 'MONEY':\n",
    "        info['Money'].append(ent.text)\n",
    "    elif ent.label_ == 'PERCENT':\n",
    "        info['Percentages'].append(ent.text)\n",
    "\n",
    "# Display extracted information\n",
    "print(\"\\nExtracted Information:\\n\")\n",
    "for category, items in info.items():\n",
    "    if items:\n",
    "        print(f\"{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  - {item}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 2: Resume/CV Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from a resume\n",
    "\n",
    "resume = \"\"\"John Smith\n",
    "Email: john.smith@email.com\n",
    "Phone: +1-555-123-4567\n",
    "\n",
    "EXPERIENCE:\n",
    "Senior Data Scientist at Google (2020-2024)\n",
    "- Led machine learning projects in Mountain View, California\n",
    "- Managed a team of 5 engineers\n",
    "- Increased model accuracy by 23%\n",
    "\n",
    "Data Analyst at Microsoft (2018-2020)\n",
    "- Analyzed user data in Redmond, Washington\n",
    "- Worked with Python, SQL, and TensorFlow\n",
    "\n",
    "EDUCATION:\n",
    "Master of Science in Computer Science\n",
    "Stanford University (2016-2018)\n",
    "GPA: 3.9/4.0\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(resume)\n",
    "\n",
    "print(\"Resume Parsing:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract key entities\n",
    "resume_info = {\n",
    "    'Name': [],\n",
    "    'Organizations': [],\n",
    "    'Locations': [],\n",
    "    'Dates': [],\n",
    "    'Skills': [],  # Note: Would need custom NER for skills\n",
    "}\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON' and len(ent.text.split()) >= 2:  # Full names\n",
    "        resume_info['Name'].append(ent.text)\n",
    "    elif ent.label_ == 'ORG':\n",
    "        resume_info['Organizations'].append(ent.text)\n",
    "    elif ent.label_ == 'GPE':\n",
    "        resume_info['Locations'].append(ent.text)\n",
    "    elif ent.label_ == 'DATE':\n",
    "        resume_info['Dates'].append(ent.text)\n",
    "\n",
    "# Display parsed information\n",
    "for category, items in resume_info.items():\n",
    "    if items:\n",
    "        unique_items = list(set(items))  # Remove duplicates\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in unique_items:\n",
    "            print(f\"  • {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 3: Entity-Based Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity-based summary\n",
    "\n",
    "def entity_summary(text):\n",
    "    \"\"\"\n",
    "    Create a summary based on most frequently mentioned entities.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Count entity mentions\n",
    "    entity_counts = Counter()\n",
    "    entity_types = {}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        entity_counts[ent.text] += 1\n",
    "        entity_types[ent.text] = ent.label_\n",
    "    \n",
    "    # Get most common entities\n",
    "    top_entities = entity_counts.most_common(10)\n",
    "    \n",
    "    return top_entities, entity_types\n",
    "\n",
    "# Example article\n",
    "article = \"\"\"Tesla CEO Elon Musk announced that Tesla will open a new manufacturing \n",
    "facility in Berlin, Germany. The facility will produce electric vehicles and batteries. \n",
    "Musk stated that Tesla has invested $5 billion in the project. The Berlin factory \n",
    "is expected to create 10,000 jobs. Tesla already operates factories in Fremont, \n",
    "California and Shanghai, China. The company aims to produce 500,000 vehicles \n",
    "annually at the Berlin facility by 2025.\"\"\"\n",
    "\n",
    "top_entities, entity_types = entity_summary(article)\n",
    "\n",
    "print(\"Entity-Based Summary:\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Article: {article[:100]}...\\n\")\n",
    "print(\"-\"*80)\n",
    "print(\"\\nKey Entities (by frequency):\\n\")\n",
    "print(f\"{'Entity':<25} {'Type':<15} {'Mentions'}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for entity, count in top_entities:\n",
    "    etype = entity_types[entity]\n",
    "    print(f\"{entity:<25} {etype:<15} {count}\")\n",
    "\n",
    "print(\"\\nSummary: This article primarily discusses Tesla and Elon Musk's announcement\")\n",
    "print(\"about a new facility in Berlin, Germany.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Topics <a id='advanced'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Linking and Disambiguation\n",
    "\n",
    "Sometimes the same name can refer to different entities (e.g., \"Apple\" as company vs. fruit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of entity ambiguity\n",
    "\n",
    "ambiguous_texts = [\n",
    "    \"I ate an apple for breakfast.\",  # apple = fruit\n",
    "    \"Apple released a new iPhone today.\",  # Apple = company\n",
    "    \"Washington led the troops to victory.\",  # Washington = person\n",
    "    \"I visited Washington last summer.\",  # Washington = place\n",
    "]\n",
    "\n",
    "print(\"Entity Disambiguation Examples:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for text in ambiguous_texts:\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    \n",
    "    entities_found = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    if entities_found:\n",
    "        print(\"Entities:\")\n",
    "        for ent_text, ent_label in entities_found:\n",
    "            print(f\"  - '{ent_text}' → {ent_label}\")\n",
    "    else:\n",
    "        print(\"  No entities detected (likely common noun)\")\n",
    "    \n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of nested entities\n",
    "\n",
    "text = \"The CEO of Apple Inc., Tim Cook, announced the new product.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Nested Entities Example:\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Text: '{text}'\\n\")\n",
    "print(\"Entities Found:\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"  '{ent.text}' → {ent.label_} (chars {ent.start_char}-{ent.end_char})\")\n",
    "\n",
    "print(\"\\nNote: 'Apple Inc.' is an organization, 'Tim Cook' is a person\")\n",
    "print(\"Some NER systems can identify that Tim Cook is CEO of Apple Inc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-World Examples <a id='real-world'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a collection of news articles\n",
    "\n",
    "articles = [\n",
    "    \"\"\"Amazon CEO Jeff Bezos announced a $2 billion investment in climate change \n",
    "    initiatives. The program will focus on renewable energy projects in Seattle.\"\"\",\n",
    "    \n",
    "    \"\"\"Microsoft reported quarterly earnings of $51.7 billion, up 18% from last year. \n",
    "    CEO Satya Nadella credited cloud computing growth in the Q4 2024 results.\"\"\",\n",
    "    \n",
    "    \"\"\"Tesla stock rose 7.2% after Elon Musk announced plans to build a factory in \n",
    "    Texas. The $1.1 billion facility will create 5,000 jobs by December 2025.\"\"\"\n",
    "]\n",
    "\n",
    "print(\"News Article Collection Analysis:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_entities = []\n",
    "entity_by_article = {}\n",
    "\n",
    "for i, article in enumerate(articles, 1):\n",
    "    doc = nlp(article)\n",
    "    \n",
    "    article_entities = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in article_entities:\n",
    "            article_entities[ent.label_] = []\n",
    "        article_entities[ent.label_].append(ent.text)\n",
    "        all_entities.append((ent.text, ent.label_))\n",
    "    \n",
    "    entity_by_article[f\"Article {i}\"] = article_entities\n",
    "    \n",
    "    print(f\"\\nArticle {i}:\")\n",
    "    print(f\"Text: {article[:80]}...\")\n",
    "    print(\"Entities:\")\n",
    "    for etype, entities in article_entities.items():\n",
    "        print(f\"  {etype}: {', '.join(entities)}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "# Overall statistics\n",
    "entity_counter = Counter([ent[1] for ent in all_entities])\n",
    "\n",
    "print(\"\\nOverall Entity Type Distribution:\")\n",
    "for etype, count in entity_counter.most_common():\n",
    "    print(f\"  {etype}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entity distribution\n",
    "\n",
    "# Create DataFrame\n",
    "df_entities = pd.DataFrame(entity_counter.most_common(), \n",
    "                           columns=['Entity Type', 'Count'])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_entities['Entity Type'], df_entities['Count'], \n",
    "        color='steelblue', alpha=0.8)\n",
    "plt.xlabel('Entity Type', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "plt.title('Named Entity Distribution in News Articles', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEntity Type Distribution:\")\n",
    "print(df_entities.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive NER analysis function\n",
    "\n",
    "def analyze_text_entities(text, verbose=True):\n",
    "    \"\"\"\n",
    "    Comprehensive named entity analysis.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        verbose (bool): Print detailed output\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Collect all entities\n",
    "    entities = [(ent.text, ent.label_, ent.start_char, ent.end_char) \n",
    "                for ent in doc.ents]\n",
    "    \n",
    "    # Group by type\n",
    "    by_type = {}\n",
    "    for ent_text, ent_label, _, _ in entities:\n",
    "        if ent_label not in by_type:\n",
    "            by_type[ent_label] = []\n",
    "        by_type[ent_label].append(ent_text)\n",
    "    \n",
    "    # Count frequencies\n",
    "    entity_freq = Counter([e[0] for e in entities])\n",
    "    type_freq = Counter([e[1] for e in entities])\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'total_entities': len(entities),\n",
    "        'unique_entities': len(set([e[0] for e in entities])),\n",
    "        'entity_types': len(type_freq),\n",
    "        'entities_by_type': by_type,\n",
    "        'most_common_entity': entity_freq.most_common(1)[0] if entity_freq else None,\n",
    "        'most_common_type': type_freq.most_common(1)[0] if type_freq else None,\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nNER Analysis Results:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total entities found: {stats['total_entities']}\")\n",
    "        print(f\"Unique entities: {stats['unique_entities']}\")\n",
    "        print(f\"Entity types: {stats['entity_types']}\")\n",
    "        \n",
    "        if stats['most_common_entity']:\n",
    "            entity, count = stats['most_common_entity']\n",
    "            print(f\"Most mentioned: '{entity}' ({count} times)\")\n",
    "        \n",
    "        print(\"\\nEntities by type:\")\n",
    "        for etype, entities_list in sorted(by_type.items()):\n",
    "            print(f\"  {etype}: {len(entities_list)} ({', '.join(set(entities_list))})\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Test the function\n",
    "test_text = \"\"\"On January 10, 2024, Microsoft CEO Satya Nadella met with \n",
    "OpenAI's Sam Altman in San Francisco to discuss a $10 billion partnership. \n",
    "The deal represents the largest AI investment to date. Shares of Microsoft \n",
    "rose 4.5% while the NASDAQ gained 1.2%.\"\"\"\n",
    "\n",
    "print(\"Testing Comprehensive NER Analysis:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Text: {test_text}\\n\")\n",
    "\n",
    "results = analyze_text_entities(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "✅ **Introduction to NER**: What named entities are and why they matter  \n",
    "✅ **NER with NLTK**: Basic entity recognition using `ne_chunk()`  \n",
    "✅ **NER with spaCy**: More accurate and comprehensive entity recognition  \n",
    "✅ **Entity Types**: PERSON, ORG, GPE, MONEY, DATE, TIME, etc.  \n",
    "✅ **Applications**: News analysis, resume parsing, text summarization  \n",
    "✅ **Advanced Topics**: Entity disambiguation, nested entities  \n",
    "✅ **Real-World Examples**: Analyzing collections of documents\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **NER is essential** for extracting structured information from unstructured text\n",
    "2. **spaCy > NLTK** for production NER applications\n",
    "   - More accurate\n",
    "   - Recognizes more entity types\n",
    "   - Better handling of complex entities\n",
    "3. **Entity types vary** by domain and use case\n",
    "4. **Challenges remain**:\n",
    "   - Entity disambiguation (\"Apple\" = company vs. fruit)\n",
    "   - Nested entities\n",
    "   - Domain-specific entities\n",
    "   - New/emerging entities\n",
    "5. **Applications are diverse**:\n",
    "   - Information extraction\n",
    "   - Question answering\n",
    "   - Content recommendation\n",
    "   - Knowledge graph construction\n",
    "\n",
    "### Common NER Patterns:\n",
    "\n",
    "- **Person**: [First Name] [Last Name]\n",
    "- **Organization**: [Company Name] [Inc./Corp./Ltd.]\n",
    "- **Location**: [City], [State/Province]\n",
    "- **Date**: [Month] [Day], [Year]\n",
    "- **Money**: [$€£] [Number] [million/billion]\n",
    "\n",
    "---\n",
    "\n",
    "**Next Notebook**: `05_Word_Embeddings.ipynb` - Introduction to word embeddings and their benefits\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
