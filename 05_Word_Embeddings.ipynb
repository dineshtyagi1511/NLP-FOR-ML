{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Limitations of Traditional Methods](#limitations)\n",
    "3. [What are Word Embeddings?](#what-are-embeddings)\n",
    "4. [Benefits of Word Embeddings](#benefits)\n",
    "5. [Types of Word Embeddings](#types)\n",
    "6. [Understanding Semantic Similarity](#semantic-similarity)\n",
    "7. [Word Embedding Properties](#properties)\n",
    "8. [Getting Started with Pre-trained Embeddings](#pretrained)\n",
    "9. [Preparing for Word2Vec](#preparing)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a id='introduction'></a>\n",
    "\n",
    "So far, we've learned classical text representation methods:\n",
    "- **One-Hot Encoding**: Binary sparse vectors\n",
    "- **Bag of Words**: Count-based representation\n",
    "- **TF-IDF**: Weighted word importance\n",
    "\n",
    "### The Problem:\n",
    "\n",
    "All these methods have a critical limitation: **They don't capture semantic meaning**.\n",
    "\n",
    "```\n",
    "Traditional Methods:\n",
    "  \"king\" and \"queen\" → No relationship captured\n",
    "  \"king\" and \"monarch\" → No relationship captured\n",
    "  \"king\" and \"pizza\" → Same distance as \"king\" and \"queen\"\n",
    "\n",
    "Word Embeddings:\n",
    "  \"king\" and \"queen\" → Close in vector space (similar meaning)\n",
    "  \"king\" and \"monarch\" → Close in vector space (synonyms)\n",
    "  \"king\" and \"pizza\" → Far apart (unrelated)\n",
    "```\n",
    "\n",
    "### The Solution: Word Embeddings\n",
    "\n",
    "**Word embeddings** are dense, low-dimensional vectors that capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For word embeddings\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limitations of Traditional Methods <a id='limitations'></a>\n",
    "\n",
    "Let's revisit why traditional methods fall short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate limitations of one-hot encoding\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Related words\n",
    "words = ['king', 'queen', 'man', 'woman', 'pizza', 'burger']\n",
    "\n",
    "# Create one-hot-like representation using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(words)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix, \n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            xticklabels=words,\n",
    "            yticklabels=words,\n",
    "            cmap='coolwarm',\n",
    "            vmin=0, vmax=1,\n",
    "            square=True)\n",
    "plt.title('One-Hot Encoding: Cosine Similarity\\n(All different words have 0 similarity)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Problem: Traditional methods treat all words as equally different!\")\n",
    "print(\"'king' vs 'queen' = 0 similarity (should be high)\")\n",
    "print(\"'king' vs 'pizza' = 0 similarity (correct)\")\n",
    "print(\"\\nNo semantic relationship captured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Limitations:\n",
    "\n",
    "1. **No Semantic Similarity**\n",
    "   - \"happy\" and \"joyful\" are treated as completely different\n",
    "   - No notion of synonyms or related words\n",
    "\n",
    "2. **High Dimensionality**\n",
    "   - Vocabulary size can be 10,000-100,000+ words\n",
    "   - Each word = one dimension\n",
    "   - Extremely sparse vectors (mostly zeros)\n",
    "\n",
    "3. **No Context**\n",
    "   - \"bank\" (river bank) vs \"bank\" (financial institution)\n",
    "   - Traditional methods can't distinguish\n",
    "\n",
    "4. **Fixed Vocabulary**\n",
    "   - Can't handle out-of-vocabulary words\n",
    "   - New words = complete unknowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What are Word Embeddings? <a id='what-are-embeddings'></a>\n",
    "\n",
    "**Word embeddings** are dense vector representations of words in a continuous vector space.\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "```\n",
    "One-Hot Encoding:\n",
    "  \"king\" → [0, 0, 0, 1, 0, 0, ..., 0]  (10,000 dimensions, sparse)\n",
    "  \n",
    "Word Embedding:\n",
    "  \"king\" → [0.2, -0.5, 0.8, ..., 0.1]  (50-300 dimensions, dense)\n",
    "```\n",
    "\n",
    "### Properties:\n",
    "\n",
    "1. **Dense**: Every dimension has a meaningful value (not mostly zeros)\n",
    "2. **Low-dimensional**: Typically 50-300 dimensions (vs 10,000+ for one-hot)\n",
    "3. **Learned**: Automatically learned from data\n",
    "4. **Semantic**: Similar words have similar vectors\n",
    "\n",
    "### How are Embeddings Created?\n",
    "\n",
    "**Core Idea**: \"*You shall know a word by the company it keeps*\" - J.R. Firth\n",
    "\n",
    "Words that appear in similar contexts have similar meanings:\n",
    "\n",
    "```\n",
    "\"The cat sat on the mat\"\n",
    "\"The dog sat on the rug\"\n",
    "\n",
    "→ \"cat\" and \"dog\" appear in similar contexts\n",
    "→ Embeddings will be similar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual comparison\n",
    "\n",
    "comparison = {\n",
    "    'Aspect': ['Dimensionality', 'Sparsity', 'Semantic Meaning', 'Training Required', \n",
    "               'Memory Usage', 'Similarity Calculation'],\n",
    "    'One-Hot / BoW': ['10,000+', 'Very Sparse (99%+ zeros)', 'None', 'No', \n",
    "                      'High', 'Not meaningful'],\n",
    "    'TF-IDF': ['10,000+', 'Sparse', 'Weak (via IDF)', 'No', \n",
    "               'High', 'Document similarity only'],\n",
    "    'Word Embeddings': ['50-300', 'Dense (all values)', 'Strong', 'Yes', \n",
    "                        'Low', 'Semantic similarity']\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison)\n",
    "\n",
    "print(\"Comparison: Traditional vs Embeddings\\n\")\n",
    "print(\"=\"*100)\n",
    "print(df_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benefits of Word Embeddings <a id='benefits'></a>\n",
    "\n",
    "### 1. Semantic Similarity\n",
    "\n",
    "Similar words have similar vectors:\n",
    "- \"happy\" ≈ \"joyful\" ≈ \"pleased\"\n",
    "- \"king\" ≈ \"queen\" ≈ \"monarch\"\n",
    "\n",
    "### 2. Analogical Reasoning\n",
    "\n",
    "Amazing property: Vector arithmetic works!\n",
    "\n",
    "```\n",
    "king - man + woman ≈ queen\n",
    "Paris - France + Italy ≈ Rome\n",
    "walking - walk + swim ≈ swimming\n",
    "```\n",
    "\n",
    "### 3. Dimensionality Reduction\n",
    "\n",
    "- From 10,000+ dimensions to 50-300\n",
    "- Faster computation\n",
    "- Less memory\n",
    "- Better generalization\n",
    "\n",
    "### 4. Transfer Learning\n",
    "\n",
    "- Pre-trained on massive corpora (Wikipedia, news, books)\n",
    "- Can be used for any task\n",
    "- Don't need to train from scratch\n",
    "\n",
    "### 5. Better Performance\n",
    "\n",
    "- Improves performance on:\n",
    "  - Sentiment analysis\n",
    "  - Text classification\n",
    "  - Machine translation\n",
    "  - Question answering\n",
    "  - And more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Types of Word Embeddings <a id='types'></a>\n",
    "\n",
    "### Popular Word Embedding Models:\n",
    "\n",
    "| Model | Year | Description | Dimensions | Training Data |\n",
    "|-------|------|-------------|------------|---------------|\n",
    "| **Word2Vec** | 2013 | Predicts context from word or vice versa | 100-300 | Google News (100B words) |\n",
    "| **GloVe** | 2014 | Global word co-occurrence statistics | 50-300 | Wikipedia + Gigaword (6B tokens) |\n",
    "| **FastText** | 2016 | Subword information (handles rare words) | 100-300 | Wikipedia + Common Crawl |\n",
    "| **ELMo** | 2018 | Context-dependent (same word, different embeddings) | 1024 | 1 Billion Word Benchmark |\n",
    "| **BERT** | 2018 | Bidirectional context, transformer-based | 768-1024 | Wikipedia + BookCorpus |\n",
    "\n",
    "### In This Course:\n",
    "\n",
    "We'll focus on **Word2Vec** (next notebook) as it's:\n",
    "- Fundamental to understanding embeddings\n",
    "- Widely used\n",
    "- Relatively simple\n",
    "- Great for learning the concepts\n",
    "\n",
    "### Context-Independent vs Context-Dependent:\n",
    "\n",
    "**Context-Independent (Word2Vec, GloVe, FastText):**\n",
    "- One vector per word, regardless of context\n",
    "- \"bank\" always has the same embedding\n",
    "\n",
    "**Context-Dependent (ELMo, BERT):**\n",
    "- Different vectors based on context\n",
    "- \"I went to the bank\" (financial) ≠ \"river bank\" (shore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Semantic Similarity <a id='semantic-similarity'></a>\n",
    "\n",
    "Let's explore semantic similarity using pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained word embeddings\n",
    "# This will download if not already cached (~65 MB)\n",
    "\n",
    "print(\"Loading pre-trained Word2Vec embeddings...\")\n",
    "print(\"(This may take a minute for first-time download)\\n\")\n",
    "\n",
    "# Load a smaller pre-trained model\n",
    "# 'glove-wiki-gigaword-50' = GloVe embeddings, 50 dimensions\n",
    "embeddings = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "print(\"✓ Embeddings loaded successfully!\")\n",
    "print(f\"\\nVocabulary size: {len(embeddings)} words\")\n",
    "print(f\"Vector dimensions: {embeddings.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vector for a word\n",
    "\n",
    "word = 'king'\n",
    "vector = embeddings[word]\n",
    "\n",
    "print(f\"Word: '{word}'\")\n",
    "print(f\"\\nVector (first 10 dimensions): {vector[:10]}\")\n",
    "print(f\"Vector shape: {vector.shape}\")\n",
    "print(f\"\\nFull vector:\")\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words\n",
    "\n",
    "def find_similar_words(word, topn=10):\n",
    "    \"\"\"\n",
    "    Find most similar words to a given word.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        similar = embeddings.most_similar(word, topn=topn)\n",
    "        return similar\n",
    "    except KeyError:\n",
    "        return f\"Word '{word}' not in vocabulary\"\n",
    "\n",
    "# Test with different words\n",
    "test_words = ['king', 'happy', 'computer', 'france']\n",
    "\n",
    "print(\"Finding Similar Words:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for word in test_words:\n",
    "    similar = find_similar_words(word, topn=5)\n",
    "    \n",
    "    print(f\"\\nMost similar to '{word}':\")\n",
    "    print(f\"{'Word':<20} {'Similarity Score'}\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    for similar_word, score in similar:\n",
    "        print(f\"{similar_word:<20} {score:.4f}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity between word pairs\n",
    "\n",
    "word_pairs = [\n",
    "    ('king', 'queen'),      # Related\n",
    "    ('king', 'man'),        # Related\n",
    "    ('king', 'monarch'),    # Synonyms\n",
    "    ('happy', 'joyful'),    # Synonyms\n",
    "    ('happy', 'sad'),       # Antonyms\n",
    "    ('king', 'pizza'),      # Unrelated\n",
    "    ('cat', 'dog'),         # Same category\n",
    "    ('paris', 'france'),    # Capital-country\n",
    "]\n",
    "\n",
    "print(\"Word Pair Similarities:\\n\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Word 1':<15} {'Word 2':<15} {'Similarity':<12} {'Relationship'}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    try:\n",
    "        similarity = embeddings.similarity(word1, word2)\n",
    "        \n",
    "        # Categorize relationship\n",
    "        if similarity > 0.7:\n",
    "            relationship = \"Very similar\"\n",
    "        elif similarity > 0.5:\n",
    "            relationship = \"Similar\"\n",
    "        elif similarity > 0.3:\n",
    "            relationship = \"Somewhat related\"\n",
    "        else:\n",
    "            relationship = \"Unrelated\"\n",
    "        \n",
    "        print(f\"{word1:<15} {word2:<15} {similarity:<12.4f} {relationship}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"{word1:<15} {word2:<15} {'N/A':<12} Word not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Word Embedding Properties <a id='properties'></a>\n",
    "\n",
    "### Amazing Property: Vector Arithmetic!\n",
    "\n",
    "Word embeddings exhibit remarkable algebraic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Famous example: king - man + woman ≈ queen\n",
    "\n",
    "def analogy(word1, word2, word3, topn=5):\n",
    "    \"\"\"\n",
    "    Solve analogy: word1 is to word2 as word3 is to ___?\n",
    "    Example: king is to man as queen is to ___?\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = embeddings.most_similar(\n",
    "            positive=[word3, word2],\n",
    "            negative=[word1],\n",
    "            topn=topn\n",
    "        )\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        return f\"Word not found: {e}\"\n",
    "\n",
    "# Test classic analogies\n",
    "analogies = [\n",
    "    ('man', 'king', 'woman'),        # woman is to ? (expect: queen)\n",
    "    ('france', 'paris', 'italy'),    # italy is to ? (expect: rome)\n",
    "    ('walk', 'walking', 'swim'),     # swim is to ? (expect: swimming)\n",
    "    ('good', 'better', 'bad'),       # bad is to ? (expect: worse)\n",
    "]\n",
    "\n",
    "print(\"Word Analogy Examples:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for word1, word2, word3 in analogies:\n",
    "    result = analogy(word1, word2, word3, topn=3)\n",
    "    \n",
    "    print(f\"\\n'{word1}' is to '{word2}' as '{word3}' is to:\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    if isinstance(result, list):\n",
    "        for word, score in result:\n",
    "            print(f\"  {word:<20} (confidence: {score:.4f})\")\n",
    "    else:\n",
    "        print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word relationships\n",
    "\n",
    "# Select words to visualize\n",
    "words_to_plot = [\n",
    "    # Royalty\n",
    "    'king', 'queen', 'prince', 'princess',\n",
    "    # Gender\n",
    "    'man', 'woman', 'boy', 'girl',\n",
    "    # Countries and capitals\n",
    "    'france', 'paris', 'italy', 'rome',\n",
    "    # Animals\n",
    "    'cat', 'dog', 'bird',\n",
    "    # Emotions\n",
    "    'happy', 'sad', 'angry',\n",
    "]\n",
    "\n",
    "# Get vectors\n",
    "vectors = np.array([embeddings[word] for word in words_to_plot])\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], s=100, alpha=0.6, c='steelblue')\n",
    "\n",
    "# Add labels\n",
    "for i, word in enumerate(words_to_plot):\n",
    "    plt.annotate(word, \n",
    "                xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontsize=12,\n",
    "                fontweight='bold')\n",
    "\n",
    "plt.xlabel('First Principal Component', fontsize=12)\n",
    "plt.ylabel('Second Principal Component', fontsize=12)\n",
    "plt.title('Word Embeddings Visualization (PCA)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how related words cluster together!\")\n",
    "print(\"- Royalty words are near each other\")\n",
    "print(\"- Animals cluster together\")\n",
    "print(\"- Countries and capitals form patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Getting Started with Pre-trained Embeddings <a id='pretrained'></a>\n",
    "\n",
    "Using pre-trained embeddings for your ML tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to embeddings\n",
    "\n",
    "def text_to_embedding(text):\n",
    "    \"\"\"\n",
    "    Convert text to average word embedding.\n",
    "    This is a simple approach - more sophisticated methods exist.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Get embeddings for words that exist in vocabulary\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_vectors.append(embeddings[word])\n",
    "        except KeyError:\n",
    "            # Word not in vocabulary, skip it\n",
    "            pass\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        # No words found, return zero vector\n",
    "        return np.zeros(embeddings.vector_size)\n",
    "    \n",
    "    # Average all word vectors\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Test sentences\n",
    "sentences = [\n",
    "    \"The king rules the kingdom\",\n",
    "    \"The queen is very powerful\",\n",
    "    \"I love eating pizza\",\n",
    "    \"Dogs and cats are pets\",\n",
    "]\n",
    "\n",
    "print(\"Text to Embedding Conversion:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sentence_embeddings = []\n",
    "for sentence in sentences:\n",
    "    embedding = text_to_embedding(sentence)\n",
    "    sentence_embeddings.append(embedding)\n",
    "    \n",
    "    print(f\"Sentence: '{sentence}'\")\n",
    "    print(f\"Embedding (first 5 dims): {embedding[:5]}\")\n",
    "    print(f\"Shape: {embedding.shape}\\n\")\n",
    "\n",
    "# Calculate similarities between sentences\n",
    "print(\"\\nSentence Similarities:\\n\")\n",
    "print(f\"{'Sentence 1':<35} {'Sentence 2':<35} {'Similarity'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        sim = cosine_similarity(\n",
    "            sentence_embeddings[i].reshape(1, -1),\n",
    "            sentence_embeddings[j].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        sent1 = sentences[i][:32] + \"...\" if len(sentences[i]) > 32 else sentences[i]\n",
    "        sent2 = sentences[j][:32] + \"...\" if len(sentences[j]) > 32 else sentences[j]\n",
    "        \n",
    "        print(f\"{sent1:<35} {sent2:<35} {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Preparing for Word2Vec <a id='preparing'></a>\n",
    "\n",
    "In the next notebook, we'll dive deep into Word2Vec:\n",
    "- How it works internally\n",
    "- Training your own embeddings\n",
    "- Skip-gram vs CBOW architectures\n",
    "- Practical applications\n",
    "\n",
    "### Key Concepts to Remember:\n",
    "\n",
    "1. **Embeddings are learned, not designed**\n",
    "   - Model learns from data\n",
    "   - No manual feature engineering\n",
    "\n",
    "2. **Context matters**\n",
    "   - Words in similar contexts → similar embeddings\n",
    "   - \"You shall know a word by the company it keeps\"\n",
    "\n",
    "3. **Dense, low-dimensional**\n",
    "   - 50-300 dimensions vs 10,000+\n",
    "   - More efficient and effective\n",
    "\n",
    "4. **Capture semantic relationships**\n",
    "   - Similarity\n",
    "   - Analogies\n",
    "   - Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization: Traditional vs Embeddings\n",
    "\n",
    "# Sample words\n",
    "words_compare = ['king', 'queen', 'man', 'woman']\n",
    "\n",
    "# Get embedding vectors\n",
    "embed_vectors = np.array([embeddings[word] for word in words_compare])\n",
    "\n",
    "# Reduce to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embed_2d = tsne.fit_transform(embed_vectors)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: One-hot encoding (conceptual)\n",
    "ax1.scatter([0, 1, 2, 3], [0, 0, 0, 0], s=200, c='gray', alpha=0.5)\n",
    "for i, word in enumerate(words_compare):\n",
    "    ax1.annotate(word, (i, 0), fontsize=14, ha='center', va='bottom')\n",
    "ax1.set_xlim(-0.5, 3.5)\n",
    "ax1.set_ylim(-0.5, 0.5)\n",
    "ax1.set_title('One-Hot Encoding\\n(All words equally distant)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.text(1.5, -0.3, 'No semantic relationship', ha='center', fontsize=12, style='italic')\n",
    "\n",
    "# Right: Word embeddings\n",
    "ax2.scatter(embed_2d[:, 0], embed_2d[:, 1], s=200, c='steelblue', alpha=0.6)\n",
    "for i, word in enumerate(words_compare):\n",
    "    ax2.annotate(word, (embed_2d[i, 0], embed_2d[i, 1]), \n",
    "                fontsize=14, fontweight='bold',\n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "ax2.set_title('Word Embeddings\\n(Similar words close together)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Dimension 1', fontsize=12)\n",
    "ax2.set_ylabel('Dimension 2', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Difference:\")\n",
    "print(\"  Left:  Traditional methods treat all words as equally different\")\n",
    "print(\"  Right: Embeddings capture semantic relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "✅ **Limitations of Traditional Methods**: High dimensionality, no semantic meaning  \n",
    "✅ **What are Word Embeddings**: Dense, low-dimensional vector representations  \n",
    "✅ **Benefits**: Semantic similarity, analogical reasoning, dimensionality reduction  \n",
    "✅ **Types of Embeddings**: Word2Vec, GloVe, FastText, ELMo, BERT  \n",
    "✅ **Semantic Similarity**: Finding related words, calculating similarities  \n",
    "✅ **Amazing Properties**: Vector arithmetic (king - man + woman ≈ queen)  \n",
    "✅ **Pre-trained Embeddings**: Using pre-trained models for your tasks  \n",
    "✅ **Preparation for Word2Vec**: Understanding the foundation\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Paradigm shift** from sparse to dense representations\n",
    "2. **Semantic meaning** is captured automatically from data\n",
    "3. **Dimensionality reduction** from 10,000+ to 50-300 dimensions\n",
    "4. **Transfer learning** via pre-trained embeddings\n",
    "5. **Better performance** on downstream ML tasks\n",
    "\n",
    "### Why Word Embeddings Matter:\n",
    "\n",
    "```\n",
    "Traditional NLP Pipeline:\n",
    "  Text → [BoW/TF-IDF] → Sparse high-dim vectors → ML Model\n",
    "\n",
    "Modern NLP Pipeline:\n",
    "  Text → [Word Embeddings] → Dense low-dim vectors → ML Model\n",
    "                ↑\n",
    "         Better semantic understanding\n",
    "         More efficient computation\n",
    "         Improved performance\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next Notebook**: `06_Word2Vec.ipynb` - Deep dive into Word2Vec training and architectures\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
