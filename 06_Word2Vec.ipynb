{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec: Skip-gram and CBOW\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Word2Vec](#introduction)\n",
    "2. [Intuition Behind Word2Vec](#intuition)\n",
    "3. [CBOW Architecture](#cbow)\n",
    "4. [Skip-gram Architecture](#skipgram)\n",
    "5. [Training Word2Vec Models](#training)\n",
    "6. [Comparing CBOW vs Skip-gram](#comparison)\n",
    "7. [Advanced Topics](#advanced)\n",
    "8. [Real-World Applications](#applications)\n",
    "9. [Best Practices](#best-practices)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Word2Vec <a id='introduction'></a>\n",
    "\n",
    "**Word2Vec** is a technique for learning word embeddings introduced by Tomas Mikolov et al. at Google in 2013.\n",
    "\n",
    "### What Makes Word2Vec Special?\n",
    "\n",
    "1. **Efficient**: Can train on billions of words\n",
    "2. **Effective**: Captures semantic and syntactic relationships\n",
    "3. **Simple**: Based on neural networks with simple architecture\n",
    "4. **Widely Used**: Foundation for many modern NLP techniques\n",
    "\n",
    "### Two Architectures:\n",
    "\n",
    "Word2Vec comes in two flavors:\n",
    "\n",
    "1. **CBOW (Continuous Bag of Words)**\n",
    "   - Predicts target word from context words\n",
    "   - Faster to train\n",
    "   - Better for frequent words\n",
    "\n",
    "2. **Skip-gram**\n",
    "   - Predicts context words from target word\n",
    "   - Slower to train\n",
    "   - Better for rare words and small datasets\n",
    "\n",
    "### Key Innovation:\n",
    "\n",
    "**Distributional Hypothesis**: *\"You shall know a word by the company it keeps\"*\n",
    "\n",
    "Words appearing in similar contexts have similar meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Gensim for Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim.downloader as api\n",
    "\n",
    "# NLTK for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Intuition Behind Word2Vec <a id='intuition'></a>\n",
    "\n",
    "### The Core Idea:\n",
    "\n",
    "```\n",
    "Given a sentence:\n",
    "\"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "For the word \"fox\":\n",
    "  Context window (size=2): [quick, brown, jumps, over]\n",
    "  \n",
    "Word2Vec learns: Words with similar contexts should have similar embeddings\n",
    "```\n",
    "\n",
    "### Example:\n",
    "\n",
    "```\n",
    "Sentence 1: \"The cat sat on the mat\"\n",
    "Sentence 2: \"The dog sat on the rug\"\n",
    "\n",
    "Both \"cat\" and \"dog\" appear in context: [The ___ sat]\n",
    "‚Üí \"cat\" and \"dog\" will have similar embeddings\n",
    "```\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Slide a window** over the text\n",
    "2. **Create training pairs** (target word, context words)\n",
    "3. **Train a neural network** to predict relationships\n",
    "4. **Extract embeddings** from the hidden layer\n",
    "\n",
    "### Shallow Neural Network:\n",
    "\n",
    "```\n",
    "Input Layer ‚Üí Hidden Layer ‚Üí Output Layer\n",
    "(Vocab Size)  (Embedding)    (Vocab Size)\n",
    "   10,000    ‚Üí     300     ‚Üí    10,000\n",
    "                    ‚Üë\n",
    "              These become our\n",
    "              word embeddings!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sliding window concept\n",
    "\n",
    "def show_context_windows(sentence, window_size=2):\n",
    "    \"\"\"\n",
    "    Show how Word2Vec creates context windows.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    \n",
    "    print(f\"Sentence: '{sentence}'\")\n",
    "    print(f\"Window size: {window_size}\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n{'Target Word':<20} {'Context Words'}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for i, target_word in enumerate(words):\n",
    "        # Get context words\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(words), i + window_size + 1)\n",
    "        \n",
    "        context = [words[j] for j in range(start, end) if j != i]\n",
    "        \n",
    "        print(f\"{target_word:<20} {context}\")\n",
    "\n",
    "# Example\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "show_context_windows(sentence, window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CBOW Architecture <a id='cbow'></a>\n",
    "\n",
    "**CBOW (Continuous Bag of Words)** predicts the target word from its context.\n",
    "\n",
    "### How CBOW Works:\n",
    "\n",
    "```\n",
    "Given context: [\"quick\", \"brown\", \"jumps\", \"over\"]\n",
    "Predict:       \"fox\"\n",
    "```\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "```\n",
    "Context Words\n",
    "    ‚Üì  ‚Üì  ‚Üì  ‚Üì\n",
    "[One-Hot Vectors]\n",
    "    ‚Üì  ‚Üì  ‚Üì  ‚Üì\n",
    "[Embedding Layer] ‚Üê Weights are word embeddings\n",
    "    ‚Üì  ‚Üì  ‚Üì  ‚Üì\n",
    "  [Average/Sum]\n",
    "       ‚Üì\n",
    "[Hidden Layer]\n",
    "       ‚Üì\n",
    "[Output Layer]\n",
    "       ‚Üì\n",
    "   Softmax\n",
    "       ‚Üì\n",
    "Target Word Probability\n",
    "```\n",
    "\n",
    "### Advantages:\n",
    "- **Faster** than Skip-gram\n",
    "- **Better for frequent words**\n",
    "- **Smaller training data** requirements\n",
    "\n",
    "### Disadvantages:\n",
    "- **Less effective for rare words**\n",
    "- **Loses word order** (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple CBOW model\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"the dog runs fast\",\n",
    "    \"the fox is quick\",\n",
    "    \"the cat and dog are friends\",\n",
    "    \"a quick brown cat\",\n",
    "    \"the lazy cat sleeps\",\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "print(\"Training CBOW Model...\\n\")\n",
    "print(\"Sample sentences:\")\n",
    "for i, sent in enumerate(tokenized_sentences[:3], 1):\n",
    "    print(f\"  {i}. {' '.join(sent)}\")\n",
    "print(f\"  ... ({len(tokenized_sentences)} total sentences)\\n\")\n",
    "\n",
    "# Train CBOW model\n",
    "# sg=0 means CBOW (sg=1 is Skip-gram)\n",
    "cbow_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=50,      # Embedding dimension\n",
    "    window=2,            # Context window size\n",
    "    min_count=1,         # Minimum word frequency\n",
    "    sg=0,                # 0 = CBOW, 1 = Skip-gram\n",
    "    epochs=100,          # Training iterations\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"‚úì CBOW Model trained!\\n\")\n",
    "print(f\"Vocabulary size: {len(cbow_model.wv)}\")\n",
    "print(f\"Vector dimensions: {cbow_model.wv.vector_size}\")\n",
    "print(f\"\\nVocabulary: {list(cbow_model.wv.index_to_key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore CBOW embeddings\n",
    "\n",
    "# Get similar words\n",
    "test_words = ['dog', 'cat', 'quick']\n",
    "\n",
    "print(\"CBOW Model - Similar Words:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for word in test_words:\n",
    "    try:\n",
    "        similar = cbow_model.wv.most_similar(word, topn=3)\n",
    "        print(f\"\\nMost similar to '{word}':\")\n",
    "        for sim_word, score in similar:\n",
    "            print(f\"  {sim_word:<15} ‚Üí {score:.4f}\")\n",
    "    except KeyError:\n",
    "        print(f\"\\n'{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Skip-gram Architecture <a id='skipgram'></a>\n",
    "\n",
    "**Skip-gram** predicts context words from the target word (opposite of CBOW).\n",
    "\n",
    "### How Skip-gram Works:\n",
    "\n",
    "```\n",
    "Given target:  \"fox\"\n",
    "Predict:       [\"quick\", \"brown\", \"jumps\", \"over\"]\n",
    "```\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "```\n",
    "Target Word\n",
    "     ‚Üì\n",
    "[One-Hot Vector]\n",
    "     ‚Üì\n",
    "[Embedding Layer] ‚Üê Weights are word embeddings\n",
    "     ‚Üì\n",
    "[Hidden Layer]\n",
    "     ‚Üì\n",
    "[Output Layer]\n",
    "     ‚Üì\n",
    "  Softmax\n",
    "     ‚Üì\n",
    "Context Words Probabilities\n",
    "```\n",
    "\n",
    "### Training Pairs Created:\n",
    "\n",
    "```\n",
    "\"The quick brown fox jumps\"\n",
    "\n",
    "Target: \"fox\"\n",
    "Training pairs:\n",
    "  (fox, quick)\n",
    "  (fox, brown)\n",
    "  (fox, jumps)\n",
    "```\n",
    "\n",
    "### Advantages:\n",
    "- **Better for rare words**\n",
    "- **Better for small datasets**\n",
    "- **Captures more nuanced relationships**\n",
    "\n",
    "### Disadvantages:\n",
    "- **Slower to train**\n",
    "- **More training data needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Skip-gram model\n",
    "\n",
    "print(\"Training Skip-gram Model...\\n\")\n",
    "\n",
    "# Train Skip-gram model (sg=1)\n",
    "skipgram_model = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=50,\n",
    "    window=2,\n",
    "    min_count=1,\n",
    "    sg=1,                # 1 = Skip-gram\n",
    "    epochs=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"‚úì Skip-gram Model trained!\\n\")\n",
    "print(f\"Vocabulary size: {len(skipgram_model.wv)}\")\n",
    "print(f\"Vector dimensions: {skipgram_model.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Skip-gram embeddings\n",
    "\n",
    "print(\"Skip-gram Model - Similar Words:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for word in test_words:\n",
    "    try:\n",
    "        similar = skipgram_model.wv.most_similar(word, topn=3)\n",
    "        print(f\"\\nMost similar to '{word}':\")\n",
    "        for sim_word, score in similar:\n",
    "            print(f\"  {sim_word:<15} ‚Üí {score:.4f}\")\n",
    "    except KeyError:\n",
    "        print(f\"\\n'{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Word2Vec Models <a id='training'></a>\n",
    "\n",
    "Let's train on a larger, more realistic corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger training corpus\n",
    "\n",
    "corpus_text = \"\"\"\n",
    "Natural language processing is a subfield of artificial intelligence.\n",
    "It focuses on the interaction between computers and human language.\n",
    "Machine learning algorithms are essential for modern NLP systems.\n",
    "Deep learning has revolutionized natural language understanding.\n",
    "Word embeddings capture semantic relationships between words.\n",
    "Neural networks can learn complex patterns in text data.\n",
    "Transformers are the foundation of modern language models.\n",
    "BERT and GPT are examples of transformer-based models.\n",
    "Text classification is a common NLP task.\n",
    "Sentiment analysis determines the emotional tone of text.\n",
    "Named entity recognition identifies important entities.\n",
    "Part of speech tagging assigns grammatical categories.\n",
    "Machine translation converts text from one language to another.\n",
    "Question answering systems provide direct responses to queries.\n",
    "Text summarization creates concise versions of documents.\n",
    "Chatbots use NLP to understand and respond to users.\n",
    "Language models predict the next word in a sequence.\n",
    "Transfer learning allows models to leverage pre-trained knowledge.\n",
    "Fine-tuning adapts models to specific tasks and domains.\n",
    "Word vectors represent words as dense numerical features.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences = sent_tokenize(corpus_text)\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "tokenized_corpus = [word_tokenize(sent.lower()) for sent in sentences]\n",
    "\n",
    "print(f\"Corpus Statistics:\")\n",
    "print(f\"  Sentences: {len(tokenized_corpus)}\")\n",
    "print(f\"  Total words: {sum(len(sent) for sent in tokenized_corpus)}\")\n",
    "print(f\"  Unique words: {len(set([word for sent in tokenized_corpus for word in sent]))}\")\n",
    "\n",
    "print(f\"\\nSample sentences:\")\n",
    "for i, sent in enumerate(tokenized_corpus[:3], 1):\n",
    "    print(f\"  {i}. {' '.join(sent[:15])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both models on the corpus\n",
    "\n",
    "print(\"Training Word2Vec Models...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CBOW model\n",
    "print(\"\\n1. Training CBOW...\")\n",
    "cbow = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100,     # Increase embedding dimension\n",
    "    window=5,            # Larger context window\n",
    "    min_count=1,\n",
    "    sg=0,                # CBOW\n",
    "    epochs=100,\n",
    "    workers=4,           # Parallel processing\n",
    "    seed=42\n",
    ")\n",
    "print(\"   ‚úì CBOW trained\")\n",
    "\n",
    "# Skip-gram model\n",
    "print(\"\\n2. Training Skip-gram...\")\n",
    "skipgram = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,                # Skip-gram\n",
    "    epochs=100,\n",
    "    workers=4,\n",
    "    seed=42\n",
    ")\n",
    "print(\"   ‚úì Skip-gram trained\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nModel Statistics:\")\n",
    "print(f\"\\nCBOW:\")\n",
    "print(f\"  Vocabulary: {len(cbow.wv)} words\")\n",
    "print(f\"  Dimensions: {cbow.wv.vector_size}\")\n",
    "\n",
    "print(f\"\\nSkip-gram:\")\n",
    "print(f\"  Vocabulary: {len(skipgram.wv)} words\")\n",
    "print(f\"  Dimensions: {skipgram.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore learned embeddings\n",
    "\n",
    "test_words_nlp = ['language', 'learning', 'model', 'text']\n",
    "\n",
    "print(\"Exploring Trained Embeddings:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for word in test_words_nlp:\n",
    "    print(f\"\\nWord: '{word}'\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # CBOW similar words\n",
    "    cbow_similar = cbow.wv.most_similar(word, topn=5)\n",
    "    print(\"\\nCBOW - Most similar:\")\n",
    "    for w, score in cbow_similar:\n",
    "        print(f\"  {w:<20} {score:.4f}\")\n",
    "    \n",
    "    # Skip-gram similar words\n",
    "    sg_similar = skipgram.wv.most_similar(word, topn=5)\n",
    "    print(\"\\nSkip-gram - Most similar:\")\n",
    "    for w, score in sg_similar:\n",
    "        print(f\"  {w:<20} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test analogies\n",
    "\n",
    "print(\"Testing Word Analogies:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test if our model learned relationships\n",
    "analogies = [\n",
    "    ('machine', 'learning', 'deep'),     # deep learning\n",
    "    ('natural', 'language', 'artificial'), # artificial intelligence\n",
    "]\n",
    "\n",
    "for word1, word2, word3 in analogies:\n",
    "    print(f\"\\n'{word1}' is to '{word2}' as '{word3}' is to:\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    try:\n",
    "        # CBOW\n",
    "        cbow_result = cbow.wv.most_similar(\n",
    "            positive=[word3, word2],\n",
    "            negative=[word1],\n",
    "            topn=3\n",
    "        )\n",
    "        print(\"\\nCBOW predictions:\")\n",
    "        for w, score in cbow_result:\n",
    "            print(f\"  {w:<20} {score:.4f}\")\n",
    "        \n",
    "        # Skip-gram\n",
    "        sg_result = skipgram.wv.most_similar(\n",
    "            positive=[word3, word2],\n",
    "            negative=[word1],\n",
    "            topn=3\n",
    "        )\n",
    "        print(\"\\nSkip-gram predictions:\")\n",
    "        for w, score in sg_result:\n",
    "            print(f\"  {w:<20} {score:.4f}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing CBOW vs Skip-gram <a id='comparison'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison table\n",
    "\n",
    "comparison_data = {\n",
    "    'Aspect': [\n",
    "        'Training Speed',\n",
    "        'Best For',\n",
    "        'Rare Words',\n",
    "        'Dataset Size',\n",
    "        'Context Usage',\n",
    "        'Typical Use Case'\n",
    "    ],\n",
    "    'CBOW': [\n",
    "        'Faster',\n",
    "        'Frequent words',\n",
    "        'Less effective',\n",
    "        'Smaller datasets',\n",
    "        'Predicts target from context',\n",
    "        'Quick prototyping, large vocab'\n",
    "    ],\n",
    "    'Skip-gram': [\n",
    "        'Slower',\n",
    "        'Rare words',\n",
    "        'More effective',\n",
    "        'Any size (especially small)',\n",
    "        'Predicts context from target',\n",
    "        'Quality embeddings, rare words'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"CBOW vs Skip-gram Comparison:\\n\")\n",
    "print(\"=\"*90)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"\\nRule of Thumb:\")\n",
    "print(\"  ‚Ä¢ Use CBOW when: You have large datasets and need speed\")\n",
    "print(\"  ‚Ä¢ Use Skip-gram when: You have small datasets or care about rare words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings from both models\n",
    "\n",
    "# Select words to visualize\n",
    "words_to_viz = ['language', 'natural', 'machine', 'learning', \n",
    "                'deep', 'neural', 'text', 'model', 'word']\n",
    "\n",
    "# Get vectors from both models\n",
    "cbow_vectors = np.array([cbow.wv[word] for word in words_to_viz])\n",
    "sg_vectors = np.array([skipgram.wv[word] for word in words_to_viz])\n",
    "\n",
    "# Reduce to 2D using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "cbow_2d = tsne.fit_transform(cbow_vectors)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "sg_2d = tsne.fit_transform(sg_vectors)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# CBOW\n",
    "ax1.scatter(cbow_2d[:, 0], cbow_2d[:, 1], s=100, c='steelblue', alpha=0.6)\n",
    "for i, word in enumerate(words_to_viz):\n",
    "    ax1.annotate(word, (cbow_2d[i, 0], cbow_2d[i, 1]),\n",
    "                fontsize=12, fontweight='bold',\n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "ax1.set_title('CBOW Embeddings (t-SNE)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Skip-gram\n",
    "ax2.scatter(sg_2d[:, 0], sg_2d[:, 1], s=100, c='coral', alpha=0.6)\n",
    "for i, word in enumerate(words_to_viz):\n",
    "    ax2.annotate(word, (sg_2d[i, 0], sg_2d[i, 1]),\n",
    "                fontsize=12, fontweight='bold',\n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "ax2.set_title('Skip-gram Embeddings (t-SNE)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Both models learn similar relationships but may differ in details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Topics <a id='advanced'></a>\n",
    "\n",
    "### Negative Sampling\n",
    "\n",
    "Training Word2Vec with full softmax is computationally expensive:\n",
    "- Need to compute probabilities for entire vocabulary (10,000+ words)\n",
    "\n",
    "**Solution: Negative Sampling**\n",
    "- Instead of updating all words, update only:\n",
    "  - The target word (positive sample)\n",
    "  - A few random words (negative samples, typically 5-20)\n",
    "\n",
    "### Hierarchical Softmax\n",
    "\n",
    "Alternative to negative sampling:\n",
    "- Uses binary tree structure\n",
    "- Reduces complexity from O(V) to O(log V)\n",
    "- Better for frequent words\n",
    "\n",
    "### Subsampling Frequent Words\n",
    "\n",
    "Very frequent words (\"the\", \"is\", \"a\") provide less information:\n",
    "- Randomly skip frequent words during training\n",
    "- Speeds up training\n",
    "- Improves quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with different hyperparameters\n",
    "\n",
    "print(\"Training with Different Configurations:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "configs = [\n",
    "    {'name': 'Small window', 'window': 2, 'vector_size': 50},\n",
    "    {'name': 'Large window', 'window': 10, 'vector_size': 50},\n",
    "    {'name': 'High dimensions', 'window': 5, 'vector_size': 200},\n",
    "]\n",
    "\n",
    "models = {}\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nTraining: {config['name']}\")\n",
    "    print(f\"  Window: {config['window']}, Dimensions: {config['vector_size']}\")\n",
    "    \n",
    "    model = Word2Vec(\n",
    "        sentences=tokenized_corpus,\n",
    "        vector_size=config['vector_size'],\n",
    "        window=config['window'],\n",
    "        min_count=1,\n",
    "        sg=1,  # Skip-gram\n",
    "        epochs=50,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    models[config['name']] = model\n",
    "    print(f\"  ‚úì Training complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nComparing similar words for 'learning':\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    similar = model.wv.most_similar('learning', topn=3)\n",
    "    print(f\"\\n{name}:\")\n",
    "    for word, score in similar:\n",
    "        print(f\"  {word:<20} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-World Applications <a id='applications'></a>\n",
    "\n",
    "Using Word2Vec embeddings for practical tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 1: Document Similarity\n",
    "\n",
    "def document_vector(doc, model):\n",
    "    \"\"\"\n",
    "    Convert document to vector by averaging word vectors.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(doc.lower())\n",
    "    \n",
    "    # Get vectors for words in vocabulary\n",
    "    word_vecs = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_vecs.append(model.wv[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "    \n",
    "    return np.mean(word_vecs, axis=0)\n",
    "\n",
    "# Test documents\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Deep learning uses neural networks with multiple layers\",\n",
    "    \"Natural language processing enables computers to understand text\",\n",
    "    \"I enjoy eating pizza and pasta for dinner\"\n",
    "]\n",
    "\n",
    "# Convert to vectors\n",
    "doc_vectors = [document_vector(doc, skipgram) for doc in documents]\n",
    "\n",
    "# Calculate similarities\n",
    "print(\"Document Similarity Matrix:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    for j in range(i+1, len(documents)):\n",
    "        sim = cosine_similarity(\n",
    "            doc_vectors[i].reshape(1, -1),\n",
    "            doc_vectors[j].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        print(f\"\\nDoc {i+1} vs Doc {j+1}: {sim:.4f}\")\n",
    "        print(f\"  Doc {i+1}: {documents[i][:60]}...\")\n",
    "        print(f\"  Doc {j+1}: {documents[j][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 2: Text Classification with Word2Vec Features\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample training data\n",
    "texts = [\n",
    "    \"machine learning algorithm\",\n",
    "    \"deep neural network\",\n",
    "    \"natural language processing\",\n",
    "    \"computer vision task\",\n",
    "    \"pizza pasta dinner\",\n",
    "    \"restaurant food menu\",\n",
    "    \"cooking recipe ingredients\",\n",
    "    \"breakfast lunch dinner\"\n",
    "]\n",
    "\n",
    "labels = [0, 0, 0, 0, 1, 1, 1, 1]  # 0=Tech, 1=Food\n",
    "\n",
    "# Convert to vectors\n",
    "X = np.array([document_vector(text, skipgram) for text in texts])\n",
    "y = np.array(labels)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Test\n",
    "test_texts = [\n",
    "    \"deep learning model\",\n",
    "    \"delicious food\",\n",
    "    \"neural network training\"\n",
    "]\n",
    "\n",
    "print(\"Text Classification using Word2Vec:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for text in test_texts:\n",
    "    vec = document_vector(text, skipgram).reshape(1, -1)\n",
    "    prediction = clf.predict(vec)[0]\n",
    "    probability = clf.predict_proba(vec)[0]\n",
    "    \n",
    "    category = \"Tech\" if prediction == 0 else \"Food\"\n",
    "    confidence = max(probability) * 100\n",
    "    \n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Predicted: {category} (confidence: {confidence:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices <a id='best-practices'></a>\n",
    "\n",
    "### Training Tips:\n",
    "\n",
    "1. **Choose the Right Architecture**\n",
    "   - Large corpus ‚Üí CBOW (faster)\n",
    "   - Small corpus or rare words ‚Üí Skip-gram\n",
    "\n",
    "2. **Window Size**\n",
    "   - Smaller (2-5): Captures syntactic relationships\n",
    "   - Larger (5-10): Captures semantic/topical relationships\n",
    "\n",
    "3. **Vector Dimensions**\n",
    "   - Typical: 100-300\n",
    "   - More dimensions = more capacity, but also more data needed\n",
    "\n",
    "4. **Training Epochs**\n",
    "   - Typically 5-100 epochs\n",
    "   - Monitor convergence\n",
    "\n",
    "5. **Min Count**\n",
    "   - Filter very rare words (min_count=5 is common)\n",
    "   - Reduces vocabulary size and noise\n",
    "\n",
    "### When to Use Pre-trained vs Training Your Own:\n",
    "\n",
    "**Use Pre-trained:**\n",
    "- General domain\n",
    "- Limited training data\n",
    "- Quick prototyping\n",
    "\n",
    "**Train Your Own:**\n",
    "- Domain-specific vocabulary\n",
    "- Lots of domain-specific data\n",
    "- Need custom embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load models\n",
    "\n",
    "# Save model\n",
    "model_path = \"/tmp/word2vec_model.bin\"\n",
    "skipgram.save(model_path)\n",
    "print(f\"‚úì Model saved to: {model_path}\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = Word2Vec.load(model_path)\n",
    "print(f\"‚úì Model loaded from: {model_path}\")\n",
    "\n",
    "# Verify loaded model works\n",
    "test_word = 'language'\n",
    "similar = loaded_model.wv.most_similar(test_word, topn=3)\n",
    "\n",
    "print(f\"\\nTest: Similar words to '{test_word}':\")\n",
    "for word, score in similar:\n",
    "    print(f\"  {word:<20} {score:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Loaded model works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this comprehensive notebook, we covered:\n",
    "\n",
    "‚úÖ **Introduction to Word2Vec**: Revolutionary word embedding technique  \n",
    "‚úÖ **Core Intuition**: Distributional hypothesis - words in similar contexts  \n",
    "‚úÖ **CBOW Architecture**: Predict target from context (faster, frequent words)  \n",
    "‚úÖ **Skip-gram Architecture**: Predict context from target (better for rare words)  \n",
    "‚úÖ **Training Models**: Hands-on training with Gensim  \n",
    "‚úÖ **Comparison**: When to use CBOW vs Skip-gram  \n",
    "‚úÖ **Advanced Topics**: Negative sampling, hyperparameters  \n",
    "‚úÖ **Real-World Applications**: Document similarity, classification  \n",
    "‚úÖ **Best Practices**: Training tips and guidelines\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Word2Vec revolutionized NLP** by learning semantic representations\n",
    "2. **Two architectures**:\n",
    "   - CBOW: Fast, good for frequent words\n",
    "   - Skip-gram: Better for rare words and small datasets\n",
    "3. **Amazing properties**:\n",
    "   - Semantic similarity\n",
    "   - Vector arithmetic (king - man + woman ‚âà queen)\n",
    "   - Clustering related words\n",
    "4. **Practical applications**: Classification, similarity, feature extraction\n",
    "5. **Foundation for modern NLP**: Led to BERT, GPT, and transformers\n",
    "\n",
    "### The Word2Vec Legacy:\n",
    "\n",
    "```\n",
    "2013: Word2Vec introduced\n",
    "       ‚Üì\n",
    "2014: GloVe (improved global statistics)\n",
    "       ‚Üì\n",
    "2016: FastText (subword information)\n",
    "       ‚Üì\n",
    "2018: ELMo (context-dependent)\n",
    "       ‚Üì\n",
    "2018: BERT (bidirectional transformers)\n",
    "       ‚Üì\n",
    "2019: GPT-2, GPT-3 (massive language models)\n",
    "       ‚Üì\n",
    "Today: ChatGPT, modern LLMs\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "To continue your NLP journey:\n",
    "1. **Explore other embeddings**: GloVe, FastText\n",
    "2. **Learn transformers**: BERT, GPT architecture\n",
    "3. **Try advanced models**: Fine-tune pre-trained models\n",
    "4. **Build projects**: Apply Word2Vec to your own data\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations! üéâ\n",
    "\n",
    "You've completed the comprehensive NLP for Machine Learning tutorial series!\n",
    "\n",
    "You now understand:\n",
    "- Text preprocessing fundamentals\n",
    "- Classical text representation methods\n",
    "- POS tagging and NER\n",
    "- Modern word embeddings\n",
    "- Word2Vec architectures and training\n",
    "\n",
    "**Keep learning and building! üöÄ**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
